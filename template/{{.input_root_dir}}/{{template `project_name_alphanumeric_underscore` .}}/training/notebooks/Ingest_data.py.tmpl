# Databricks notebook source
##################################################################################
# Data Ingestion Notebook
#
# This notebook shows an example of a Data Ingestion pipeline using Delta tables.
# It is configured and can be executed as the "Ingest" task in the model_training_job workflow defined under
# ``{{template `project_name_alphanumeric_underscore` .}}/resources/model-workflow-resource.yml``
#
# Parameters:
# * env (required):                 - Environment the notebook is run in (staging, or prod). Defaults to "staging".
# * output_data_path (required)     - Path where the ingested data will be stored.
##################################################################################

# COMMAND ----------

# MAGIC %load_ext autoreload
# MAGIC %autoreload 2

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path

# COMMAND ----------

# MAGIC %pip install -r ../../requirements.txt

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1, Notebook arguments
# List of input args needed to run this notebook as a job.
# Provide them via DB widgets or notebook arguments.

# Notebook Environment
dbutils.widgets.dropdown("env", "staging", ["staging", "prod"], "Environment Name")
env = dbutils.widgets.get("env")

# Path where the ingested data will be stored.
dbutils.widgets.text(
    "output_data_path",
    "TODO",
    label="Path where the ingested data will be stored",
)

# COMMAND ----------

# DBTITLE 1, Define input and output variables
output_data_path = dbutils.widgets.get("output_data_path")

# COMMAND ----------

# MAGIC %md
# MAGIC **Data Source**:
# MAGIC
# MAGIC Prod:
# MAGIC
# MAGIC   1.first,
# MAGIC
# MAGIC   2.second,
# MAGIC
# MAGIC   3.third,
# MAGIC
# MAGIC   4.forth

# COMMAND ----------

# DBTITLE 1, Initialize Spark session
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Initialize Spark session
spark = SparkSession.builder.appName("DataIngestion").getOrCreate()

# COMMAND ----------

# MAGIC %md
# MAGIC #**Step 1: Data Loading**

# COMMAND ----------

# DBTITLE 1, Load data from multiple tables
# Constructing our target to be trained from four tables
# Load the data from the relevant tables
print("Loading data from multiple tables...")

first_df = spark.table("first_df_masked")
second_df = spark.table("second_df_masked")
third_df = spark.table("third_df_masked")
forth_df = spark.table("forth_df_masked")

print("Data tables loaded successfully!")
print(f"First table rows: {first_df.count()}")
print(f"Second table rows: {second_df.count()}")
print(f"Third table rows: {third_df.count()}")
print(f"Fourth table rows: {forth_df.count()}")

# COMMAND ----------

# MAGIC %md
# MAGIC #**Step 2: Data Exploration and Preprocessing**

# COMMAND ----------

# DBTITLE 1, Explore first dataset
print("Exploring first dataset:")
display(first_df)
print(f"First table schema:")
first_df.printSchema()

# COMMAND ----------

# DBTITLE 1, Check distinct values in target column
# Check distinct values in target column if it exists
if "target_label" in first_df.columns:
    distinct_values = first_df.select("target_label").distinct()
    print("Distinct target values:")
    distinct_values.show()

# COMMAND ----------

# MAGIC %md
# MAGIC #**Step 3: Joining the Datasets**

# COMMAND ----------

# DBTITLE 1, Join datasets using SQL query
# TODO: write the proper query to join the datasets and get a final dataframe
# Changes were made in query with comments on the line that was changed

# Example SQL query - modify according to your specific join requirements
sql_query = """
SELECT 
    f.*,
    s.*,
    t.*,
    fo.*
FROM first_df_masked f
LEFT JOIN second_df_masked s ON f.id = s.id
LEFT JOIN third_df_masked t ON f.id = t.id  
LEFT JOIN forth_df_masked fo ON f.id = fo.id
"""

print("Executing join query...")

# Step 2: Create a temporary view using the SQL query
spark.sql(f"CREATE OR REPLACE TEMP VIEW final_data_view AS {sql_query}")

# Step 3: Read the temporary view into a DataFrame
result_df = spark.table("final_data_view")

print(f"Joined dataset rows: {result_df.count()}")
print(f"Joined dataset columns: {len(result_df.columns)}")

# COMMAND ----------

# DBTITLE 1, Display joined dataset
# Display the DataFrame
print("Joined dataset preview:")
display(result_df)

# COMMAND ----------

# DBTITLE 1, Check data quality of joined dataset
# Check for null values in joined dataset
print("Data quality checks for joined dataset:")
null_counts = result_df.select([count(when(col(c).isNull(), c)).alias(c) for c in result_df.columns])
print("Null value counts:")
null_counts.show()

# Check for duplicate rows
duplicate_count = result_df.count() - result_df.dropDuplicates().count()
print(f"Number of duplicate rows: {duplicate_count}")

# COMMAND ----------

# DBTITLE 1, Database information
# MAGIC %sql
# MAGIC SHOW CATALOGS;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT current_catalog();

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT current_schema();

# COMMAND ----------

# DBTITLE 1, Save ingested data
# Save the joined data to the specified output path
print(f"Saving ingested data to: {output_data_path}")

# Write as Delta format for better performance and ACID transactions
result_df.write.mode("overwrite").format("delta").save(output_data_path)

print("Data ingestion completed successfully!")

# COMMAND ----------

# DBTITLE 1, Log ingestion metrics
# Log important metrics for monitoring
ingestion_metrics = {
    "first_table_rows": first_df.count(),
    "second_table_rows": second_df.count(),
    "third_table_rows": third_df.count(),
    "fourth_table_rows": forth_df.count(),
    "joined_rows": result_df.count(),
    "joined_columns": len(result_df.columns),
    "duplicate_rows": duplicate_count,
    "output_path": output_data_path
}

print("Ingestion Metrics:")
for key, value in ingestion_metrics.items():
    print(f"{key}: {value}") 