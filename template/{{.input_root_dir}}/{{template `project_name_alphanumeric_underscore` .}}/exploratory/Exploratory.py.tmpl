# Databricks notebook source
##################################################################################
# Exploratory Data Analysis Notebook
#
# This notebook shows an example of an Exploratory Data Analysis pipeline using Delta tables.
# It is designed for data scientists to explore and understand the dataset before model development.
#
# Parameters:
# * env (required):                 - Environment the notebook is run in (staging, or prod). Defaults to "staging".
# * data_path (required)            - Path to the data for exploration.
# * output_path (optional)          - Path where the analysis results will be stored.
##################################################################################

# COMMAND ----------

# MAGIC %load_ext autoreload
# MAGIC %autoreload 2

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path

# COMMAND ----------

# MAGIC %pip install -r ../requirements.txt

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1, Notebook arguments
# List of input args needed to run this notebook as a job.
# Provide them via DB widgets or notebook arguments.

# Notebook Environment
dbutils.widgets.dropdown("env", "staging", ["staging", "prod"], "Environment Name")
env = dbutils.widgets.get("env")

# Path to the data for exploration.
dbutils.widgets.text(
    "data_path",
    "TODO",
    label="Path to the data for exploration",
)

# Path where the analysis results will be stored.
dbutils.widgets.text(
    "output_path",
    "TODO",
    label="Path where the analysis results will be stored",
)

# COMMAND ----------

# DBTITLE 1, Define input and output variables
data_path = dbutils.widgets.get("data_path")
output_path = dbutils.widgets.get("output_path")

# COMMAND ----------

# DBTITLE 1, Load and explore data
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Initialize Spark session
spark = SparkSession.builder.appName("ExploratoryDataAnalysis").getOrCreate()

# Read data
print(f"Reading data from: {data_path}")
df = spark.read.format("delta").load(data_path)

print("Dataset Overview:")
print(f"Number of rows: {df.count()}")
print(f"Number of columns: {len(df.columns)}")
print("Columns:", df.columns)

# COMMAND ----------

# DBTITLE 1, Basic data information
# Display schema
print("Data Schema:")
df.printSchema()

# Display sample data
print("Sample Data:")
df.show(5)

# COMMAND ----------

# DBTITLE 1, Descriptive statistics
# Calculate descriptive statistics for numeric columns
numeric_columns = [field.name for field in df.schema.fields if field.dataType.typeName() in ['integer', 'double', 'float']]

if numeric_columns:
    print("Descriptive Statistics for Numeric Columns:")
    df.select(numeric_columns).describe().show()
else:
    print("No numeric columns found in the dataset.")

# COMMAND ----------

# DBTITLE 1, Missing value analysis
# Analyze missing values
print("Missing Value Analysis:")
missing_data = []

for column in df.columns:
    missing_count = df.filter(col(column).isNull()).count()
    total_count = df.count()
    missing_percentage = (missing_count / total_count) * 100 if total_count > 0 else 0
    
    missing_data.append({
        'column': column,
        'missing_count': missing_count,
        'missing_percentage': missing_percentage
    })

missing_df = spark.createDataFrame(missing_data)
missing_df.show()

# COMMAND ----------

# DBTITLE 1, Data type analysis
# Analyze data types
print("Data Type Analysis:")
data_types = []

for field in df.schema.fields:
    data_types.append({
        'column': field.name,
        'data_type': str(field.dataType)
    })

type_df = spark.createDataFrame(data_types)
type_df.show()

# COMMAND ----------

# DBTITLE 1, Categorical variable analysis
# Analyze categorical variables
categorical_columns = [field.name for field in df.schema.fields if field.dataType.typeName() == 'string']

if categorical_columns:
    print("Categorical Variable Analysis:")
    for col_name in categorical_columns:
        print(f"\nUnique values in {col_name}:")
        df.groupBy(col_name).count().orderBy(col("count"), ascending=False).show(10)
else:
    print("No categorical columns found in the dataset.")

# COMMAND ----------

# DBTITLE 1, Correlation analysis (for numeric variables)
if len(numeric_columns) > 1:
    print("Correlation Analysis:")
    
    # Convert to pandas for correlation analysis
    numeric_df = df.select(numeric_columns).toPandas()
    correlation_matrix = numeric_df.corr()
    
    print("Correlation Matrix:")
    print(correlation_matrix)
    
    # Create correlation heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Correlation Heatmap')
    plt.tight_layout()
    plt.show()

# COMMAND ----------

# DBTITLE 1, Distribution analysis
# Analyze distributions of numeric variables
if numeric_columns:
    print("Distribution Analysis:")
    
    for col_name in numeric_columns:
        print(f"\nDistribution of {col_name}:")
        
        # Convert to pandas for plotting
        col_data = df.select(col_name).toPandas()[col_name].dropna()
        
        plt.figure(figsize=(10, 6))
        plt.subplot(1, 2, 1)
        plt.hist(col_data, bins=30, alpha=0.7)
        plt.title(f'Histogram of {col_name}')
        plt.xlabel(col_name)
        plt.ylabel('Frequency')
        
        plt.subplot(1, 2, 2)
        plt.boxplot(col_data)
        plt.title(f'Box Plot of {col_name}')
        plt.ylabel(col_name)
        
        plt.tight_layout()
        plt.show()

# COMMAND ----------

# DBTITLE 1, Save analysis results
# Save the analysis results
if output_path != "TODO":
    print(f"Saving analysis results to: {output_path}")
    
    # Create analysis summary
    analysis_summary = {
        "total_rows": df.count(),
        "total_columns": len(df.columns),
        "numeric_columns": len(numeric_columns),
        "categorical_columns": len(categorical_columns),
        "columns_with_missing_values": len([d for d in missing_data if d['missing_count'] > 0])
    }
    
    # Convert to DataFrame and save
    summary_df = spark.createDataFrame([analysis_summary])
    summary_df.write.mode("overwrite").format("delta").save(output_path)
    
    print("Analysis results saved successfully!")

# COMMAND ----------

# DBTITLE 1, Summary
print("Exploratory Data Analysis Summary:")
print(f"Dataset contains {df.count()} rows and {len(df.columns)} columns")
print(f"Found {len(numeric_columns)} numeric columns and {len(categorical_columns)} categorical columns")
print(f"Columns with missing values: {len([d for d in missing_data if d['missing_count'] > 0])}")

print("\nKey Insights:")
for d in missing_data:
    if d['missing_percentage'] > 10:
        print(f"- {d['column']} has {d['missing_percentage']:.1f}% missing values")

if numeric_columns:
    print(f"- Numeric columns available for correlation analysis: {numeric_columns}")

print("Exploratory data analysis completed!") 