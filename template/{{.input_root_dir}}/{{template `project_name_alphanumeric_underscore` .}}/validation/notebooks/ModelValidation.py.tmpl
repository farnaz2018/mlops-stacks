# Databricks notebook source
##################################################################################
# Model Validation Notebook
##
# This notebook uses mlflow model validation API to run model validation after training and registering a Random Forest
# classifier model in model registry, before deploying it to the {{- if (eq .input_include_models_in_unity_catalog "no") }}"Production" stage{{else}} "champion" alias{{end -}}.
#
# It runs as part of CD and by an automated model training job -> validation -> deployment job defined under ``{{template `project_name_alphanumeric_underscore` .}}/resources/model-workflow-resource.yml``
#
# This validation notebook is specifically configured for Random Forest classification models and includes:
# - Classification-specific metrics (accuracy, precision, recall, F1-score, balanced accuracy)
# - Custom validation thresholds for classification performance
# - Baseline model comparison capabilities
#
# Parameters:
#
# * env                                     - Name of the environment the notebook is run in (staging, or prod). Defaults to "prod".
# * `run_mode`                              - The `run_mode` defines whether model validation is enabled or not. It can be one of the three values:
#                                             * `disabled` : Do not run the model validation notebook.
#                                             * `dry_run`  : Run the model validation notebook. Ignore failed model validation rules and proceed to move
#                                                            model to the {{- if (eq .input_include_models_in_unity_catalog "no") }}"Production" stage{{else}} "champion" alias{{end -}}.
#                                             * `enabled`  : Run the model validation notebook. Move model to the {{- if (eq .input_include_models_in_unity_catalog "no") }} "Production" stage {{else}} "champion" alias {{end -}} only if all model validation
#                                                            rules are passing.
{{- if (eq .input_include_models_in_unity_catalog "no") }}
# * enable_baseline_comparison              - Whether to load the current registered "Production" stage model as baseline.
{{else}}
# * enable_baseline_comparison              - Whether to load the current registered "champion" model as baseline.
{{end -}}
#                                             Baseline model is a requirement for relative change and absolute change validation thresholds.
# * validation_input                        - Validation input. Please refer to data parameter in mlflow.evaluate documentation https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate
# * model_type                              - A string describing the model type. Set to "classifier" for Random Forest classification models.
#                                             Please refer to model_type parameter in mlflow.evaluate documentation https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate
# * targets                                 - The string name of a column from data that contains evaluation labels (e.g., "target").
#                                             Please refer to targets parameter in mlflow.evaluate documentation https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate
# * custom_metrics_loader_function          - Specifies the name of the function in {{ .input_project_name }}/validation/validation.py that returns custom metrics.
# * validation_thresholds_loader_function   - Specifies the name of the function in {{ .input_project_name }}/validation/validation.py that returns model validation thresholds.
#
# For details on mlflow evaluate API, see doc https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate
# For details and examples about performing model validation, see the Model Validation documentation https://mlflow.org/docs/latest/models.html#model-validation
#
##################################################################################

# COMMAND ----------

# MAGIC %load_ext autoreload
# MAGIC %autoreload 2

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path

# COMMAND ----------

# MAGIC %pip install -r ../../requirements.txt

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

import os
notebook_path =  '/Workspace/' + os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())
%cd $notebook_path
%cd ../

# COMMAND ----------

dbutils.widgets.text(
    "experiment_name",
    "/dev-{{template `experiment_base_name` .}}",
    "Experiment Name",
)
dbutils.widgets.dropdown("run_mode", "disabled", ["disabled", "dry_run", "enabled"], "Run Mode")
dbutils.widgets.dropdown("enable_baseline_comparison", "false", ["true", "false"], "Enable Baseline Comparison")
dbutils.widgets.text("validation_input", "SELECT * FROM delta.`dbfs:/databricks-datasets/nyctaxi-with-zipcodes/subsampled`", "Validation Input")
dbutils.widgets.text("model_type", "classifier", "Model Type")
dbutils.widgets.text("targets", "target", "Targets")
dbutils.widgets.text("custom_metrics_loader_function", "custom_metrics", "Custom Metrics Loader Function")
dbutils.widgets.text("validation_thresholds_loader_function", "validation_thresholds", "Validation Thresholds Loader Function")
dbutils.widgets.text("evaluator_config_loader_function", "evaluator_config", "Evaluator Config Loader Function")
{{- if (eq .input_include_models_in_unity_catalog "no") }}
dbutils.widgets.text("model_name", "dev-{{template `model_name` .}}", "Model Name")
{{else}}
dbutils.widgets.text("model_name", "dev.{{ .input_schema_name }}.{{template `model_name` .}}", "Full (Three-Level) Model Name")
{{end -}}
dbutils.widgets.text("model_version", "", "Candidate Model Version")

# COMMAND ----------
run_mode = dbutils.widgets.get("run_mode").lower()
assert run_mode == "disabled" or run_mode == "dry_run" or run_mode == "enabled"

if run_mode == "disabled":
    print(
        "Model validation is in DISABLED mode. Exit model validation without blocking model deployment."
    )
    dbutils.notebook.exit(0)
dry_run = run_mode == "dry_run"

if dry_run:
    print(
        "Model validation is in DRY_RUN mode. Validation threshold validation failures will not block model deployment."
    )
else:
    print(
        "Model validation is in ENABLED mode. Validation threshold validation failures will block model deployment."
    )

# COMMAND ----------

# MAGIC %md
# MAGIC #**Random Forest Model Validation**
# MAGIC
# MAGIC This validation notebook is specifically configured for Random Forest classification models. The validation process includes:
# MAGIC
# MAGIC **Metrics Evaluated:**
# MAGIC - **Accuracy**: Overall classification accuracy (threshold: ≥75%)
# MAGIC - **Balanced Accuracy**: Accuracy for imbalanced datasets (threshold: ≥70%)
# MAGIC - **Precision**: Precision score with weighted average (threshold: ≥70%)
# MAGIC - **Recall**: Recall score with weighted average (threshold: ≥70%)
# MAGIC - **F1 Score**: F1 score with weighted average (threshold: ≥70%)
# MAGIC - **Log Loss**: Logarithmic loss (threshold: ≤0.5)
# MAGIC
# MAGIC **Validation Process:**
# MAGIC 1. Load the trained Random Forest model from MLflow Model Registry
# MAGIC 2. Evaluate the model on validation data
# MAGIC 3. Compare against baseline model (if enabled)
# MAGIC 4. Check all metrics against defined thresholds
# MAGIC 5. Log validation results and artifacts
# MAGIC 6. Update model description with validation status
# MAGIC

# COMMAND ----------

import importlib
import mlflow
import os
import tempfile
import traceback
from mlflow.tracking.client import MlflowClient
{{ if (eq .input_include_models_in_unity_catalog "no") }}
client = MlflowClient()
{{else}}
client = MlflowClient(registry_uri="databricks-uc")
mlflow.set_registry_uri('databricks-uc')
{{ end }}
# set experiment
experiment_name = dbutils.widgets.get("experiment_name")
mlflow.set_experiment(experiment_name)
# set model evaluation parameters that can be inferred from the job
model_uri = dbutils.jobs.taskValues.get("Train", "model_uri", debugValue="")
model_name = dbutils.jobs.taskValues.get("Train", "model_name", debugValue="")
model_version = dbutils.jobs.taskValues.get("Train", "model_version", debugValue="")

if model_uri == "":
    model_name = dbutils.widgets.get("model_name")
    model_version = dbutils.widgets.get("model_version")
    model_uri = "models:/" + model_name + "/" + model_version
{{ if (eq .input_include_models_in_unity_catalog "no") }}
baseline_model_uri = "models:/" + model_name + "/Production"
{{else}}
baseline_model_uri = "models:/" + model_name + "@champion"
{{ end }}
evaluators = "default"
assert model_uri != "", "model_uri notebook parameter must be specified"
assert model_name != "", "model_name notebook parameter must be specified"
assert model_version != "", "model_version notebook parameter must be specified"

# COMMAND ----------

# take input
enable_baseline_comparison = dbutils.widgets.get("enable_baseline_comparison")


assert enable_baseline_comparison == "true" or enable_baseline_comparison == "false"
enable_baseline_comparison = enable_baseline_comparison == "true"

validation_input = dbutils.widgets.get("validation_input")
assert validation_input
data = spark.sql(validation_input)

model_type = dbutils.widgets.get("model_type")
targets = dbutils.widgets.get("targets")
assert model_type
assert targets

custom_metrics_loader_function_name = dbutils.widgets.get("custom_metrics_loader_function")
validation_thresholds_loader_function_name = dbutils.widgets.get("validation_thresholds_loader_function")
evaluator_config_loader_function_name = dbutils.widgets.get("evaluator_config_loader_function")
assert custom_metrics_loader_function_name
assert validation_thresholds_loader_function_name
assert evaluator_config_loader_function_name
custom_metrics_loader_function = getattr(
    importlib.import_module("validation"), custom_metrics_loader_function_name
)
validation_thresholds_loader_function = getattr(
    importlib.import_module("validation"), validation_thresholds_loader_function_name
)
evaluator_config_loader_function = getattr(
    importlib.import_module("validation"), evaluator_config_loader_function_name
)
custom_metrics = custom_metrics_loader_function()
validation_thresholds = validation_thresholds_loader_function()
evaluator_config = evaluator_config_loader_function()

# COMMAND ----------

# helper methods
def get_run_link(run_info):
    return "[Run](#mlflow/experiments/{0}/runs/{1})".format(
        run_info.experiment_id, run_info.run_id
    )


def get_training_run(model_name, model_version):
    version = client.get_model_version(model_name, model_version)
    return mlflow.get_run(run_id=version.run_id)


def generate_run_name(training_run):
    return None if not training_run else training_run.info.run_name + "-validation"


def generate_description(training_run):
    return (
        None
        if not training_run
        else "Model Training Details: {0}\n".format(get_run_link(training_run.info))
    )


def log_to_model_description(run, success):
    run_link = get_run_link(run.info)
    description = client.get_model_version(model_name, model_version).description
    status = "SUCCESS" if success else "FAILURE"
    if description != "":
        description += "\n\n---\n\n"
    description += "Model Validation Status: {0}\nValidation Details: {1}".format(
        status, run_link
    )
    client.update_model_version(
        name=model_name, version=model_version, description=description
    )

{{ if (eq .input_include_feature_store `yes`) }}

from datetime import timedelta, timezone
import math
import pyspark.sql.functions as F
from pyspark.sql.types import IntegerType


def rounded_unix_timestamp(dt, num_minutes=15):
    """
    Ceilings datetime dt to interval num_minutes, then returns the unix timestamp.
    """
    nsecs = dt.minute * 60 + dt.second + dt.microsecond * 1e-6
    delta = math.ceil(nsecs / (60 * num_minutes)) * (60 * num_minutes) - nsecs
    return int((dt + timedelta(seconds=delta)).replace(tzinfo=timezone.utc).timestamp())


rounded_unix_timestamp_udf = F.udf(rounded_unix_timestamp, IntegerType())


def rounded_taxi_data(taxi_data_df):
    # Round the taxi data timestamp to 15 and 30 minute intervals so we can join with the pickup and dropoff features
    # respectively.
    taxi_data_df = (
        taxi_data_df.withColumn(
            "rounded_pickup_datetime",
            F.to_timestamp(
                rounded_unix_timestamp_udf(
                    taxi_data_df["tpep_pickup_datetime"], F.lit(15)
                )
            ),
        )
        .withColumn(
            "rounded_dropoff_datetime",
            F.to_timestamp(
                rounded_unix_timestamp_udf(
                    taxi_data_df["tpep_dropoff_datetime"], F.lit(30)
                )
            ),
        )
        .drop("tpep_pickup_datetime")
        .drop("tpep_dropoff_datetime")
    )
    taxi_data_df.createOrReplaceTempView("taxi_data")
    return taxi_data_df




# COMMAND ----------



training_run = get_training_run(model_name, model_version)

# run evaluate
with mlflow.start_run(
    run_name=generate_run_name(training_run),
    description=generate_description(training_run),
) as run, tempfile.TemporaryDirectory() as tmp_dir:
    validation_thresholds_file = os.path.join(tmp_dir, "validation_thresholds.txt")
    with open(validation_thresholds_file, "w") as f:
        if validation_thresholds:
            for metric_name in validation_thresholds:
                f.write(
                    "{0:30}  {1}\n".format(
                        metric_name, str(validation_thresholds[metric_name])
                    )
                )
    mlflow.log_artifact(validation_thresholds_file)

    try:
        eval_result = mlflow.evaluate(
            model=model_uri,
            data=data,
            targets=targets,
            model_type=model_type,
            evaluators=evaluators,
            validation_thresholds=validation_thresholds,
            custom_metrics=custom_metrics,
            baseline_model=None
            if not enable_baseline_comparison
            else baseline_model_uri,
            evaluator_config=evaluator_config,
        )
        metrics_file = os.path.join(tmp_dir, "metrics.txt")
        with open(metrics_file, "w") as f:
            f.write(
                "{0:30}  {1:30}  {2}\n".format("metric_name", "candidate", "baseline")
            )
            for metric in eval_result.metrics:
                candidate_metric_value = str(eval_result.metrics[metric])
                baseline_metric_value = "N/A"
                if metric in eval_result.baseline_model_metrics:
                    mlflow.log_metric(
                        "baseline_" + metric, eval_result.baseline_model_metrics[metric]
                    )
                    baseline_metric_value = str(
                        eval_result.baseline_model_metrics[metric]
                    )
                f.write(
                    "{0:30}  {1:30}  {2}\n".format(
                        metric, candidate_metric_value, baseline_metric_value
                    )
                )
        mlflow.log_artifact(metrics_file)
        
        # Log additional Random Forest specific validation information
        print("Random Forest Model Validation Results:")
        print("=" * 50)
        for metric in eval_result.metrics:
            candidate_metric_value = eval_result.metrics[metric]
            baseline_metric_value = "N/A"
            if metric in eval_result.baseline_model_metrics:
                baseline_metric_value = eval_result.baseline_model_metrics[metric]
            print(f"{metric:25}: {candidate_metric_value:8.4f} (baseline: {baseline_metric_value})")
        
        # Log validation summary
        validation_summary = {
            "model_type": "RandomForestClassifier",
            "validation_status": "PASSED",
            "total_metrics_evaluated": len(eval_result.metrics),
            "validation_timestamp": mlflow.utils.time.now()
        }
        
        # Save validation summary
        import json
        validation_summary_file = os.path.join(tmp_dir, "validation_summary.json")
        with open(validation_summary_file, "w") as f:
            json.dump(validation_summary, f, indent=2)
        mlflow.log_artifact(validation_summary_file)
        
        log_to_model_description(run, True)
        {{ if (eq .input_include_models_in_unity_catalog "yes") }}
        # Assign "challenger" alias to indicate model version has passed validation checks
        print("Validation checks passed. Assigning 'challenger' alias to model version.")
        client.set_registered_model_alias(model_name, "challenger", model_version)
        {{ end }}
    except Exception as err:
        log_to_model_description(run, False)
        error_file = os.path.join(tmp_dir, "error.txt")
        with open(error_file, "w") as f:
            f.write("Random Forest Model Validation failed: " + str(err) + "\n")
            f.write("Model Type: RandomForestClassifier\n")
            f.write("Validation Timestamp: " + str(mlflow.utils.time.now()) + "\n")
            f.write("=" * 50 + "\n")
            f.write(traceback.format_exc())
        mlflow.log_artifact(error_file)
        
        # Log failed validation summary
        failed_validation_summary = {
            "model_type": "RandomForestClassifier",
            "validation_status": "FAILED",
            "error_message": str(err),
            "validation_timestamp": mlflow.utils.time.now()
        }
        
        import json
        failed_summary_file = os.path.join(tmp_dir, "failed_validation_summary.json")
        with open(failed_summary_file, "w") as f:
            json.dump(failed_validation_summary, f, indent=2)
        mlflow.log_artifact(failed_summary_file)
        
        if not dry_run:
            raise err
        else:
            print(
                "Random Forest model validation failed in DRY_RUN. It will not block model deployment."
            )
