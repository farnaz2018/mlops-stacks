# Updating ML code in production

[(back to main README)](../README.md)

**NOTE**: This page assumes that your MLOps team has already configured CI/CD and deployed initial
ML resources, per the [MLOps setup guide](mlops-setup.md).

## Table of contents
* [Opening a pull request](#opening-a-pull-request)
* [Viewing test status and debug logs](#viewing-test-status-and-debug-logs)
* [Merging your pull request](#merging-your-pull-request)
* [Next steps](#next-steps)

## Opening a pull request

To push your updated ML code to production, [open a pull request](https://learn.microsoft.com/en-us/azure/devops/repos/git/pull-requests?view=azure-devops&tabs=browser#create-a-pull-request) against the remote Git repo containing the current project.

**NOTE**: the default tests provided in this repo require that you use a pull
request branch on the Git repo for the current project, rather than opening a pull request from a fork
of the Git repo. Support for running tests against pull requests from repo forks
is planned for the future.

## Viewing test status and debug logs
Opening a pull request will trigger a 
{{- if (eq .input_setup_cicd_and_project `CICD_and_Project`) }}
[Azure DevOps Pipeline](../.azure/devops-pipelines/{{ .input_project_name }}-tests-ci.yml)
{{- else }}
workflow
{{- end }} 
that runs unit and integration tests for the model training (and feature engineering if added) pipeline on Databricks against a test dataset.
You can view test status and debug logs from the pull request UI, and push new commits to your pull request branch
to address any test failures.
The integration test runs the model training notebook in the staging workspace, training, validating,
and registering a new model version in 
{{ if (eq .input_include_models_in_unity_catalog `no`) }} the workspace model registry
{{- else -}} UC
{{end}}.
The fitted model along with its metrics and params
will also be logged to an MLflow run. To debug failed integration test runs, click into the Databricks job run
URL printed in the test logs. The executed notebook of the job run will contain a link to the MLflow model training run. You can also use the Experiments page in the workspace
to view training metrics or fetch and debug the model as needed.

## Merging your pull request
Once tests pass on your pull request, get your pull request reviewed and approved by a teammate,
and then merge it into the upstream repo.

## Next Steps
{{- if (eq .input_default_branch .input_release_branch) }}
After merging your pull request, subsequent runs of the model training and batch inference
jobs in staging will automatically use your updated ML code.

You may want to wait to confirm that
the staging jobs succeed, then repeat the workflow above to open a pull request against the
`{{ .input_release_branch }}` branch to promote your ML code to production. Once your pull request against `{{ .input_release_branch }}`
merges, production jobs will also automatically include your changes. 

{{- else }}
After merging your pull request, subsequent runs of the model training and batch inference
jobs in staging and production will automatically use your updated ML code.
{{- end }}

You can track the state of the ML pipelines for the current project from the MLflow registered model UI. 
{{ if (eq .input_setup_cicd_and_project `CICD_and_Project`) }}
Links:
{{ if (eq .input_include_models_in_unity_catalog `no`) }}
* [Staging workspace registered model]({{template `databricks_staging_workspace_host` .}}/ml/models/staging-{{template `model_name` .}})
* [Prod workspace registered model]({{template `databricks_prod_workspace_host` .}}/ml/models/prod-{{template `model_name` .}})
{{- else -}} 
* [Staging model in UC]({{template `databricks_staging_workspace_host` .}}/explore/data/models/staging/{{.input_project_name}}/{{template `model_name` .}})
* [Prod model in UC]({{template `databricks_prod_workspace_host` .}}/explore/data/models/prod/{{.input_project_name}}/{{template `model_name` .}})
{{end}}. 
{{end}}

In both the staging and prod workspaces, the MLflow registered model contains links to:
* The model versions produced through automated retraining
* The Git repository containing the ML code run in the training and inference pipelines
* The recurring training job that produces new model versions using the latest ML code and data
* The model deployment CD workflow that takes model versions produced by the training job and deploys them for inference
* The recurring batch inference job that uses the currently-deployed model version to score a dataset
